{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.layers import Activation, BatchNormalization, Add, Reshape, DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 28709\n",
    "train_y = []\n",
    "train_x = []\n",
    "for i in range(n):\n",
    "    train_y.append([])\n",
    "    train_x.append([])\n",
    "\n",
    "count = 0\n",
    "with open(sys.argv[1], newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        if(row[0] != \"label\"):\n",
    "            train_y[count].append(row[0])\n",
    "            train_x[count].append(row[1])\n",
    "            count += 1\n",
    "x = []\n",
    "for i in range(len(train_x)):\n",
    "    x.append(train_x[i][0].split(' '))\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(x[0])):\n",
    "        x[i][j] = float(x[i][j])\n",
    "x = np.array(x)\n",
    "y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x.reshape(n,48,48,1)\n",
    "y = y.reshape(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"img_train.npy\",x)\n",
    "np.save(\"img_label.npy\",y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 48, 48, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 12, 12, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 6, 6, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 64)          2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 3, 3, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 96)          6240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 3, 3, 96)          384       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 3, 3, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 1, 1, 7)           231       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 1, 1, 7)           0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 21,751\n",
      "Trainable params: 20,727\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def relu6(x):\n",
    "    \"\"\"Relu 6\n",
    "    \"\"\"\n",
    "    return K.relu(x, max_value=6.0)\n",
    "\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    return LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, alpha, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        alpha: Integer, width multiplier.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    # Depth\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "    # Width\n",
    "    cchannel = int(filters * alpha)\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    x = LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "    x = Conv2D(cchannel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    if r:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        alpha: Integer, width multiplier.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, alpha, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, alpha, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def MobileNetv2(input_shape, k, alpha=1.0):\n",
    "    \"\"\"MobileNetv2\n",
    "    This function defines a MobileNetv2 architectures.\n",
    "    # Arguments\n",
    "        input_shape: An integer or tuple/list of 3 integers, shape\n",
    "            of input tensor.\n",
    "        k: Integer, number of classes.\n",
    "        alpha: Integer, width multiplier, better in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4].\n",
    "    # Returns\n",
    "        MobileNetv2 model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    #first_filters = _make_divisible(32 * alpha, 8)\n",
    "    first_filters = 16\n",
    "    x = _conv_block(inputs, first_filters, (3, 3), strides=(2, 2))\n",
    "\n",
    "    x = _inverted_residual_block(x, 16, (3, 3), t=1, alpha=alpha, strides=1, n=1)\n",
    "    x = _inverted_residual_block(x, 32, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 96, (3, 3), t=3, alpha=alpha, strides=1, n=2)\n",
    "    x = _inverted_residual_block(x, 96, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 320, (3, 3), t=6, alpha=alpha, strides=1, n=1)\n",
    "\n",
    "    #if alpha > 1.0:\n",
    "        #last_filters = _make_divisible(1280 * alpha, 8)\n",
    "    #else:\n",
    "        #last_filters = 1280\n",
    "    last_filters = 32\n",
    "    x = _conv_block(x, last_filters, (1, 1), strides=(1, 1))\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, last_filters))(x)\n",
    "    x = Dropout(0.3, name='Dropout')(x)\n",
    "    x = Conv2D(k, (1, 1), padding='same')(x)\n",
    "\n",
    "    x = Activation('softmax', name='softmax')(x)\n",
    "    output = Reshape((k,))(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    # plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = MobileNetv2((48, 48, 1), 7, 1.0)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.load(\"img_train.npy\")\n",
    "y = np.load(\"img_label.npy\")\n",
    "\n",
    "x_train = x[0:int(28709*0.8)]/255\n",
    "y_train = y[0:int(28709*0.8)]\n",
    "x_val = x[int(28709*0.8): 28709]/255\n",
    "y_val = y[int(28709*0.8):28709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'Adam',\n",
    "              metrics=['accuracy'])\n",
    "#print(MODEL.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 1.7809 - acc: 0.2624 - val_loss: 1.8765 - val_acc: 0.2447\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 119s 80ms/step - loss: 1.7097 - acc: 0.3037 - val_loss: 1.7653 - val_acc: 0.3018\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.6567 - acc: 0.3396 - val_loss: 1.5897 - val_acc: 0.3762\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 119s 80ms/step - loss: 1.6100 - acc: 0.3663 - val_loss: 1.5726 - val_acc: 0.3838\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.5773 - acc: 0.3846 - val_loss: 1.5026 - val_acc: 0.4187\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.5496 - acc: 0.3973 - val_loss: 1.5760 - val_acc: 0.3845\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.5277 - acc: 0.4074 - val_loss: 1.4578 - val_acc: 0.4357\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.5117 - acc: 0.4154 - val_loss: 1.4734 - val_acc: 0.4328\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4998 - acc: 0.4209 - val_loss: 1.4772 - val_acc: 0.4298\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4902 - acc: 0.4252 - val_loss: 1.4287 - val_acc: 0.4549\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4796 - acc: 0.4289 - val_loss: 1.4158 - val_acc: 0.4549\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4727 - acc: 0.4331 - val_loss: 1.4262 - val_acc: 0.4549\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4649 - acc: 0.4360 - val_loss: 1.5137 - val_acc: 0.4119\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4576 - acc: 0.4403 - val_loss: 1.5578 - val_acc: 0.4188\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4533 - acc: 0.4420 - val_loss: 1.4008 - val_acc: 0.4730\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4496 - acc: 0.4437 - val_loss: 1.5484 - val_acc: 0.4044\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4441 - acc: 0.4456 - val_loss: 1.3700 - val_acc: 0.4810\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4413 - acc: 0.4476 - val_loss: 1.4174 - val_acc: 0.4631\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4365 - acc: 0.4492 - val_loss: 1.3944 - val_acc: 0.4645\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4329 - acc: 0.4500 - val_loss: 1.3744 - val_acc: 0.4697\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4266 - acc: 0.4536 - val_loss: 1.4133 - val_acc: 0.4589\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4243 - acc: 0.4550 - val_loss: 1.3992 - val_acc: 0.4702\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 119s 79ms/step - loss: 1.4232 - acc: 0.4555 - val_loss: 1.3623 - val_acc: 0.4822\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 118s 79ms/step - loss: 1.4199 - acc: 0.4561 - val_loss: 1.3504 - val_acc: 0.4861\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 118s 78ms/step - loss: 1.4151 - acc: 0.4585 - val_loss: 1.3963 - val_acc: 0.4648\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    "    )\n",
    "\n",
    "datagen.fit(x_train)\n",
    "train_generator = datagen.flow(x_train,y_train,batch_size = 200)\n",
    "#learning_rate_function = ReduceLROnPlateau(monitor='val_acc',patience=2,epsilon=0.00001,verbose=1,factor=0.2)\n",
    "train_history2 = model.fit_generator(train_generator, steps_per_epoch=1500,epochs=25,verbose=1,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "n = 7178\n",
    "test_x = []\n",
    "for i in range(n):\n",
    "    test_x.append([])\n",
    "\n",
    "count = 0\n",
    "with open('test.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        if(row[0] != \"id\"):\n",
    "            test_x[count].append(row[1])\n",
    "            count += 1\n",
    "x_test = []\n",
    "for i in range(len(test_x)):\n",
    "    x_test.append(test_x[i][0].split(' '))\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(x_test[0])):\n",
    "        x_test[i][j] = float(x_test[i][j])\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape(7178,48,48,1)\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_model = load_model(\"hw3_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_predict = hw3_model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_y = np_utils.to_categorical(hw3_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "7178/7178 [==============================] - 1s 169us/step - loss: 0.1171 - acc: 0.9615\n",
      "Epoch 2/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1157 - acc: 0.9610\n",
      "Epoch 3/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1084 - acc: 0.9678\n",
      "Epoch 4/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1130 - acc: 0.9635\n",
      "Epoch 5/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1174 - acc: 0.9638\n",
      "Epoch 6/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1237 - acc: 0.9606\n",
      "Epoch 7/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1206 - acc: 0.9607\n",
      "Epoch 8/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1143 - acc: 0.9622\n",
      "Epoch 9/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1296 - acc: 0.9558\n",
      "Epoch 10/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1154 - acc: 0.9629\n",
      "Epoch 11/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1110 - acc: 0.9650\n",
      "Epoch 12/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1188 - acc: 0.9618\n",
      "Epoch 13/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1180 - acc: 0.9613\n",
      "Epoch 14/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1233 - acc: 0.9596\n",
      "Epoch 15/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1197 - acc: 0.9610\n",
      "Epoch 16/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1222 - acc: 0.9627\n",
      "Epoch 17/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1214 - acc: 0.9600\n",
      "Epoch 18/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1208 - acc: 0.9606\n",
      "Epoch 19/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1171 - acc: 0.9609\n",
      "Epoch 20/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1198 - acc: 0.9607\n",
      "Epoch 21/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1162 - acc: 0.9641\n",
      "Epoch 22/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1198 - acc: 0.9629\n",
      "Epoch 23/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1136 - acc: 0.9629\n",
      "Epoch 24/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1172 - acc: 0.9649\n",
      "Epoch 25/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1104 - acc: 0.9639\n",
      "Epoch 26/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1115 - acc: 0.9649\n",
      "Epoch 27/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1144 - acc: 0.9627\n",
      "Epoch 28/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1126 - acc: 0.9621\n",
      "Epoch 29/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1152 - acc: 0.9631\n",
      "Epoch 30/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1122 - acc: 0.9642\n",
      "Epoch 31/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1103 - acc: 0.9636\n",
      "Epoch 32/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1141 - acc: 0.9625\n",
      "Epoch 33/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1172 - acc: 0.9600\n",
      "Epoch 34/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1142 - acc: 0.9593\n",
      "Epoch 35/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1114 - acc: 0.9652\n",
      "Epoch 36/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1145 - acc: 0.9636\n",
      "Epoch 37/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1151 - acc: 0.9641\n",
      "Epoch 38/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1109 - acc: 0.9661\n",
      "Epoch 39/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1260 - acc: 0.9596\n",
      "Epoch 40/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1179 - acc: 0.9634\n",
      "Epoch 41/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1134 - acc: 0.9673\n",
      "Epoch 42/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1184 - acc: 0.9625\n",
      "Epoch 43/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1217 - acc: 0.9572\n",
      "Epoch 44/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1111 - acc: 0.9667\n",
      "Epoch 45/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1135 - acc: 0.9628\n",
      "Epoch 46/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1138 - acc: 0.9654\n",
      "Epoch 47/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1082 - acc: 0.9634\n",
      "Epoch 48/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1118 - acc: 0.9638\n",
      "Epoch 49/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1125 - acc: 0.9622\n",
      "Epoch 50/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1116 - acc: 0.9629\n",
      "Epoch 51/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1095 - acc: 0.9636\n",
      "Epoch 52/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1240 - acc: 0.9571\n",
      "Epoch 53/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1155 - acc: 0.9614\n",
      "Epoch 54/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1185 - acc: 0.9607\n",
      "Epoch 55/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1156 - acc: 0.9614\n",
      "Epoch 56/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1103 - acc: 0.9634\n",
      "Epoch 57/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1144 - acc: 0.9632\n",
      "Epoch 58/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1160 - acc: 0.9620\n",
      "Epoch 59/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1131 - acc: 0.9621\n",
      "Epoch 60/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1148 - acc: 0.9636\n",
      "Epoch 61/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1127 - acc: 0.9628\n",
      "Epoch 62/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1185 - acc: 0.9600\n",
      "Epoch 63/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1112 - acc: 0.9667\n",
      "Epoch 64/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1158 - acc: 0.9661\n",
      "Epoch 65/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1139 - acc: 0.9648\n",
      "Epoch 66/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1179 - acc: 0.9618\n",
      "Epoch 67/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1223 - acc: 0.9597\n",
      "Epoch 68/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1210 - acc: 0.9615\n",
      "Epoch 69/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1183 - acc: 0.9617\n",
      "Epoch 70/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1220 - acc: 0.9592\n",
      "Epoch 71/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1199 - acc: 0.9599\n",
      "Epoch 72/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1155 - acc: 0.9614\n",
      "Epoch 73/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1095 - acc: 0.9673\n",
      "Epoch 74/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1178 - acc: 0.9621\n",
      "Epoch 75/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1116 - acc: 0.9639\n",
      "Epoch 76/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 77/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1075 - acc: 0.9654\n",
      "Epoch 78/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1170 - acc: 0.9629\n",
      "Epoch 79/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1175 - acc: 0.9603\n",
      "Epoch 80/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1077 - acc: 0.9650\n",
      "Epoch 81/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1112 - acc: 0.9648\n",
      "Epoch 82/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1082 - acc: 0.9667\n",
      "Epoch 83/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1092 - acc: 0.9663\n",
      "Epoch 84/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1110 - acc: 0.9657\n",
      "Epoch 85/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1139 - acc: 0.9635\n",
      "Epoch 86/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1214 - acc: 0.9599\n",
      "Epoch 87/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1204 - acc: 0.9590\n",
      "Epoch 88/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1206 - acc: 0.9621\n",
      "Epoch 89/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1199 - acc: 0.9620\n",
      "Epoch 90/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1142 - acc: 0.9607\n",
      "Epoch 91/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1163 - acc: 0.9621\n",
      "Epoch 92/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1246 - acc: 0.9599\n",
      "Epoch 93/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1191 - acc: 0.9617\n",
      "Epoch 94/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1171 - acc: 0.9606\n",
      "Epoch 95/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1174 - acc: 0.9625\n",
      "Epoch 96/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1145 - acc: 0.9642\n",
      "Epoch 97/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1179 - acc: 0.9586\n",
      "Epoch 98/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1140 - acc: 0.9648\n",
      "Epoch 99/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1131 - acc: 0.9627\n",
      "Epoch 100/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1117 - acc: 0.9627\n",
      "Epoch 101/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1151 - acc: 0.9622\n",
      "Epoch 102/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1105 - acc: 0.9645\n",
      "Epoch 103/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1132 - acc: 0.9636\n",
      "Epoch 104/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1143 - acc: 0.9636\n",
      "Epoch 105/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1146 - acc: 0.9654\n",
      "Epoch 106/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1183 - acc: 0.9588\n",
      "Epoch 107/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1132 - acc: 0.9628\n",
      "Epoch 108/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1115 - acc: 0.9628\n",
      "Epoch 109/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1129 - acc: 0.9649\n",
      "Epoch 110/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1198 - acc: 0.9614\n",
      "Epoch 111/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1195 - acc: 0.9622\n",
      "Epoch 112/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1130 - acc: 0.9621\n",
      "Epoch 113/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1201 - acc: 0.9613\n",
      "Epoch 114/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1185 - acc: 0.9628\n",
      "Epoch 115/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1182 - acc: 0.9592\n",
      "Epoch 116/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1092 - acc: 0.9653\n",
      "Epoch 117/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1191 - acc: 0.9614\n",
      "Epoch 118/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1074 - acc: 0.9664\n",
      "Epoch 119/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1200 - acc: 0.9589\n",
      "Epoch 120/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1125 - acc: 0.9638\n",
      "Epoch 121/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1128 - acc: 0.9638\n",
      "Epoch 122/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1125 - acc: 0.9657\n",
      "Epoch 123/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1199 - acc: 0.9620\n",
      "Epoch 124/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1096 - acc: 0.9648\n",
      "Epoch 125/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1148 - acc: 0.9629\n",
      "Epoch 126/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1146 - acc: 0.9634\n",
      "Epoch 127/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1081 - acc: 0.9650\n",
      "Epoch 128/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1168 - acc: 0.9592\n",
      "Epoch 129/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1146 - acc: 0.9625\n",
      "Epoch 130/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1153 - acc: 0.9638\n",
      "Epoch 131/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1161 - acc: 0.9629\n",
      "Epoch 132/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1135 - acc: 0.9639\n",
      "Epoch 133/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1177 - acc: 0.9597\n",
      "Epoch 134/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1233 - acc: 0.9593\n",
      "Epoch 135/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1181 - acc: 0.9599\n",
      "Epoch 136/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1176 - acc: 0.9620\n",
      "Epoch 137/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1090 - acc: 0.9659\n",
      "Epoch 138/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1167 - acc: 0.9617\n",
      "Epoch 139/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1197 - acc: 0.9618\n",
      "Epoch 140/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1125 - acc: 0.9646\n",
      "Epoch 141/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1209 - acc: 0.9593\n",
      "Epoch 142/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1122 - acc: 0.9645\n",
      "Epoch 143/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1082 - acc: 0.9657\n",
      "Epoch 144/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1155 - acc: 0.9607\n",
      "Epoch 145/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1173 - acc: 0.9628\n",
      "Epoch 146/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1135 - acc: 0.9624\n",
      "Epoch 147/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1059 - acc: 0.9664\n",
      "Epoch 148/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1201 - acc: 0.9613\n",
      "Epoch 149/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1086 - acc: 0.9660\n",
      "Epoch 150/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1175 - acc: 0.9614\n",
      "Epoch 151/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1207 - acc: 0.9576\n",
      "Epoch 152/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1112 - acc: 0.9639\n",
      "Epoch 153/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1148 - acc: 0.9625\n",
      "Epoch 154/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1155 - acc: 0.9625\n",
      "Epoch 155/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1117 - acc: 0.9638\n",
      "Epoch 156/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1155 - acc: 0.9622\n",
      "Epoch 157/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1122 - acc: 0.9629\n",
      "Epoch 158/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1109 - acc: 0.9657\n",
      "Epoch 159/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1134 - acc: 0.9628\n",
      "Epoch 160/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1120 - acc: 0.9642\n",
      "Epoch 161/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1129 - acc: 0.9639\n",
      "Epoch 162/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1151 - acc: 0.9641\n",
      "Epoch 163/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1092 - acc: 0.9620\n",
      "Epoch 164/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1133 - acc: 0.9629\n",
      "Epoch 165/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1111 - acc: 0.9639\n",
      "Epoch 166/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1113 - acc: 0.9642\n",
      "Epoch 167/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1126 - acc: 0.9609\n",
      "Epoch 168/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1087 - acc: 0.9664\n",
      "Epoch 169/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1166 - acc: 0.9595\n",
      "Epoch 170/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1101 - acc: 0.9631\n",
      "Epoch 171/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1159 - acc: 0.9625\n",
      "Epoch 172/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1071 - acc: 0.9664\n",
      "Epoch 173/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1125 - acc: 0.9632\n",
      "Epoch 174/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1140 - acc: 0.9631\n",
      "Epoch 175/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1079 - acc: 0.9649\n",
      "Epoch 176/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1162 - acc: 0.9620\n",
      "Epoch 177/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1118 - acc: 0.9629\n",
      "Epoch 178/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1089 - acc: 0.9639\n",
      "Epoch 179/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1137 - acc: 0.9624\n",
      "Epoch 180/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1189 - acc: 0.9578\n",
      "Epoch 181/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1164 - acc: 0.9618\n",
      "Epoch 182/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1184 - acc: 0.9622\n",
      "Epoch 183/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1141 - acc: 0.9602\n",
      "Epoch 184/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1169 - acc: 0.9606\n",
      "Epoch 185/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1147 - acc: 0.9650\n",
      "Epoch 186/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1103 - acc: 0.9654\n",
      "Epoch 187/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1151 - acc: 0.9607\n",
      "Epoch 188/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1186 - acc: 0.9625\n",
      "Epoch 189/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1110 - acc: 0.9632\n",
      "Epoch 190/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1170 - acc: 0.9634\n",
      "Epoch 191/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1144 - acc: 0.9628\n",
      "Epoch 192/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1182 - acc: 0.9593\n",
      "Epoch 193/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1242 - acc: 0.9568\n",
      "Epoch 194/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1127 - acc: 0.9628\n",
      "Epoch 195/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1230 - acc: 0.9588\n",
      "Epoch 196/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1200 - acc: 0.9610\n",
      "Epoch 197/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1173 - acc: 0.9585\n",
      "Epoch 198/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1093 - acc: 0.9657\n",
      "Epoch 199/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1159 - acc: 0.9609\n",
      "Epoch 200/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1105 - acc: 0.9657\n",
      "Epoch 201/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1189 - acc: 0.9590\n",
      "Epoch 202/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1112 - acc: 0.9645\n",
      "Epoch 203/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1169 - acc: 0.9606\n",
      "Epoch 204/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1109 - acc: 0.9613\n",
      "Epoch 205/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1151 - acc: 0.9609\n",
      "Epoch 206/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1135 - acc: 0.9631\n",
      "Epoch 207/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1083 - acc: 0.9657\n",
      "Epoch 208/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1206 - acc: 0.9607\n",
      "Epoch 209/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1119 - acc: 0.9642\n",
      "Epoch 210/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1051 - acc: 0.9674\n",
      "Epoch 211/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1113 - acc: 0.9643\n",
      "Epoch 212/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1093 - acc: 0.9660\n",
      "Epoch 213/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1013 - acc: 0.9685\n",
      "Epoch 214/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1088 - acc: 0.9642\n",
      "Epoch 215/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1100 - acc: 0.9657\n",
      "Epoch 216/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1190 - acc: 0.9604\n",
      "Epoch 217/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1091 - acc: 0.9634\n",
      "Epoch 218/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1121 - acc: 0.9642\n",
      "Epoch 219/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1140 - acc: 0.9632\n",
      "Epoch 220/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1130 - acc: 0.9638\n",
      "Epoch 221/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1166 - acc: 0.9607\n",
      "Epoch 222/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1113 - acc: 0.9654\n",
      "Epoch 223/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1184 - acc: 0.9614\n",
      "Epoch 224/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1104 - acc: 0.9643\n",
      "Epoch 225/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1116 - acc: 0.9645\n",
      "Epoch 226/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1114 - acc: 0.9634\n",
      "Epoch 227/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1160 - acc: 0.9622\n",
      "Epoch 228/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1159 - acc: 0.9606\n",
      "Epoch 229/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1053 - acc: 0.9617\n",
      "Epoch 230/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1173 - acc: 0.9606\n",
      "Epoch 231/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1099 - acc: 0.9657\n",
      "Epoch 232/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1143 - acc: 0.9613\n",
      "Epoch 233/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1171 - acc: 0.9613\n",
      "Epoch 234/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1218 - acc: 0.9589\n",
      "Epoch 235/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1153 - acc: 0.9624\n",
      "Epoch 236/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1083 - acc: 0.9645\n",
      "Epoch 237/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1153 - acc: 0.9615\n",
      "Epoch 238/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1078 - acc: 0.9635\n",
      "Epoch 239/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1130 - acc: 0.9642\n",
      "Epoch 240/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1127 - acc: 0.9604\n",
      "Epoch 241/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1099 - acc: 0.9648\n",
      "Epoch 242/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1019 - acc: 0.9645\n",
      "Epoch 243/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1082 - acc: 0.9661\n",
      "Epoch 244/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1124 - acc: 0.9609\n",
      "Epoch 245/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1127 - acc: 0.9639\n",
      "Epoch 246/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1103 - acc: 0.9656\n",
      "Epoch 247/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1081 - acc: 0.9635\n",
      "Epoch 248/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1124 - acc: 0.9621\n",
      "Epoch 249/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1125 - acc: 0.9657\n",
      "Epoch 250/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1235 - acc: 0.9603\n",
      "Epoch 251/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1082 - acc: 0.9652\n",
      "Epoch 252/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1193 - acc: 0.9610\n",
      "Epoch 253/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1128 - acc: 0.9642\n",
      "Epoch 254/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1139 - acc: 0.9638\n",
      "Epoch 255/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1116 - acc: 0.9641\n",
      "Epoch 256/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1113 - acc: 0.9638\n",
      "Epoch 257/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1125 - acc: 0.9629\n",
      "Epoch 258/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1106 - acc: 0.9660\n",
      "Epoch 259/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1166 - acc: 0.9618\n",
      "Epoch 260/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1150 - acc: 0.9642\n",
      "Epoch 261/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1079 - acc: 0.9634\n",
      "Epoch 262/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1087 - acc: 0.9638\n",
      "Epoch 263/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1077 - acc: 0.9692\n",
      "Epoch 264/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1129 - acc: 0.9632\n",
      "Epoch 265/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1098 - acc: 0.9625\n",
      "Epoch 266/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1085 - acc: 0.9652\n",
      "Epoch 267/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1116 - acc: 0.9629\n",
      "Epoch 268/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1178 - acc: 0.9602\n",
      "Epoch 269/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1101 - acc: 0.9639\n",
      "Epoch 270/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1211 - acc: 0.9603\n",
      "Epoch 271/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1139 - acc: 0.9609\n",
      "Epoch 272/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1170 - acc: 0.9618\n",
      "Epoch 273/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1101 - acc: 0.9648\n",
      "Epoch 274/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1118 - acc: 0.9632\n",
      "Epoch 275/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1117 - acc: 0.9653\n",
      "Epoch 276/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1134 - acc: 0.9635\n",
      "Epoch 277/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1143 - acc: 0.9635\n",
      "Epoch 278/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1128 - acc: 0.9660\n",
      "Epoch 279/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1194 - acc: 0.9599\n",
      "Epoch 280/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1095 - acc: 0.9649\n",
      "Epoch 281/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1181 - acc: 0.9593\n",
      "Epoch 282/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1105 - acc: 0.9642\n",
      "Epoch 283/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1118 - acc: 0.9632\n",
      "Epoch 284/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1169 - acc: 0.9622\n",
      "Epoch 285/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1136 - acc: 0.9617\n",
      "Epoch 286/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1099 - acc: 0.9675\n",
      "Epoch 287/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1190 - acc: 0.9597\n",
      "Epoch 288/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1169 - acc: 0.9606\n",
      "Epoch 289/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1084 - acc: 0.9621\n",
      "Epoch 290/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1098 - acc: 0.9642\n",
      "Epoch 291/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1090 - acc: 0.9641\n",
      "Epoch 292/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1063 - acc: 0.9666\n",
      "Epoch 293/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1127 - acc: 0.9614\n",
      "Epoch 294/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1143 - acc: 0.9624\n",
      "Epoch 295/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1075 - acc: 0.9663\n",
      "Epoch 296/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1012 - acc: 0.9673\n",
      "Epoch 297/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1065 - acc: 0.9654\n",
      "Epoch 298/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1057 - acc: 0.9663\n",
      "Epoch 299/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1089 - acc: 0.9642\n",
      "Epoch 300/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1183 - acc: 0.9614\n",
      "Epoch 301/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1079 - acc: 0.9668\n",
      "Epoch 302/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1064 - acc: 0.9654\n",
      "Epoch 303/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1064 - acc: 0.9671\n",
      "Epoch 304/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1153 - acc: 0.9635\n",
      "Epoch 305/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1120 - acc: 0.9609\n",
      "Epoch 306/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1181 - acc: 0.9597\n",
      "Epoch 307/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1059 - acc: 0.9663\n",
      "Epoch 308/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1076 - acc: 0.9648\n",
      "Epoch 309/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1106 - acc: 0.9648\n",
      "Epoch 310/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1086 - acc: 0.9646\n",
      "Epoch 311/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1106 - acc: 0.9642\n",
      "Epoch 312/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1103 - acc: 0.9614\n",
      "Epoch 313/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1130 - acc: 0.9621\n",
      "Epoch 314/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1077 - acc: 0.9653\n",
      "Epoch 315/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1164 - acc: 0.9614\n",
      "Epoch 316/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1145 - acc: 0.9611\n",
      "Epoch 317/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1156 - acc: 0.9615\n",
      "Epoch 318/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1182 - acc: 0.9596\n",
      "Epoch 319/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1106 - acc: 0.9641\n",
      "Epoch 320/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1164 - acc: 0.9585\n",
      "Epoch 321/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1085 - acc: 0.9652\n",
      "Epoch 322/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1049 - acc: 0.9677\n",
      "Epoch 323/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1027 - acc: 0.9702\n",
      "Epoch 324/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1150 - acc: 0.9625\n",
      "Epoch 325/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1132 - acc: 0.9621\n",
      "Epoch 326/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1067 - acc: 0.9670\n",
      "Epoch 327/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1102 - acc: 0.9645\n",
      "Epoch 328/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1154 - acc: 0.9627\n",
      "Epoch 329/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1053 - acc: 0.9654\n",
      "Epoch 330/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1114 - acc: 0.9643\n",
      "Epoch 331/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1121 - acc: 0.9648\n",
      "Epoch 332/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1058 - acc: 0.9673\n",
      "Epoch 333/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1134 - acc: 0.9620\n",
      "Epoch 334/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1053 - acc: 0.9654\n",
      "Epoch 335/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1214 - acc: 0.9586\n",
      "Epoch 336/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1122 - acc: 0.9636\n",
      "Epoch 337/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1102 - acc: 0.9624\n",
      "Epoch 338/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1006 - acc: 0.9694\n",
      "Epoch 339/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1069 - acc: 0.9670\n",
      "Epoch 340/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1127 - acc: 0.9634\n",
      "Epoch 341/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1071 - acc: 0.9682\n",
      "Epoch 342/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1109 - acc: 0.9634\n",
      "Epoch 343/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1081 - acc: 0.9652\n",
      "Epoch 344/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1117 - acc: 0.9611\n",
      "Epoch 345/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1112 - acc: 0.9629\n",
      "Epoch 346/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1103 - acc: 0.9649\n",
      "Epoch 347/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1145 - acc: 0.9618\n",
      "Epoch 348/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1127 - acc: 0.9602\n",
      "Epoch 349/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1041 - acc: 0.9673\n",
      "Epoch 350/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1049 - acc: 0.9642\n",
      "Epoch 351/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1016 - acc: 0.9684\n",
      "Epoch 352/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1058 - acc: 0.9646\n",
      "Epoch 353/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1108 - acc: 0.9618\n",
      "Epoch 354/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1039 - acc: 0.9652\n",
      "Epoch 355/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1044 - acc: 0.9661\n",
      "Epoch 356/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1093 - acc: 0.9621\n",
      "Epoch 357/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1094 - acc: 0.9654\n",
      "Epoch 358/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1070 - acc: 0.9646\n",
      "Epoch 359/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1107 - acc: 0.9625\n",
      "Epoch 360/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1120 - acc: 0.9642\n",
      "Epoch 361/450\n",
      "7178/7178 [==============================] - 1s 147us/step - loss: 0.1034 - acc: 0.9663\n",
      "Epoch 362/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1094 - acc: 0.9638\n",
      "Epoch 363/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1103 - acc: 0.9648\n",
      "Epoch 364/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1135 - acc: 0.9602\n",
      "Epoch 365/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1168 - acc: 0.9610\n",
      "Epoch 366/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1004 - acc: 0.9664\n",
      "Epoch 367/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1078 - acc: 0.9664\n",
      "Epoch 368/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1047 - acc: 0.9639\n",
      "Epoch 369/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1175 - acc: 0.9609\n",
      "Epoch 370/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1110 - acc: 0.9652\n",
      "Epoch 371/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1088 - acc: 0.9661\n",
      "Epoch 372/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1161 - acc: 0.9629\n",
      "Epoch 373/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1175 - acc: 0.9607\n",
      "Epoch 374/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1064 - acc: 0.9656\n",
      "Epoch 375/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1059 - acc: 0.9659\n",
      "Epoch 376/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1134 - acc: 0.9628\n",
      "Epoch 377/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1098 - acc: 0.9628\n",
      "Epoch 378/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1081 - acc: 0.9649\n",
      "Epoch 379/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1101 - acc: 0.9653\n",
      "Epoch 380/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1046 - acc: 0.9652\n",
      "Epoch 381/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1064 - acc: 0.9632\n",
      "Epoch 382/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1164 - acc: 0.9618\n",
      "Epoch 383/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1087 - acc: 0.9629\n",
      "Epoch 384/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1017 - acc: 0.9684\n",
      "Epoch 385/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1089 - acc: 0.9653\n",
      "Epoch 386/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1051 - acc: 0.9674\n",
      "Epoch 387/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1094 - acc: 0.9656\n",
      "Epoch 388/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1148 - acc: 0.9629\n",
      "Epoch 389/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1102 - acc: 0.9632\n",
      "Epoch 390/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1139 - acc: 0.9609\n",
      "Epoch 391/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1080 - acc: 0.9666\n",
      "Epoch 392/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1109 - acc: 0.9624\n",
      "Epoch 393/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1169 - acc: 0.9615\n",
      "Epoch 394/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1061 - acc: 0.9639\n",
      "Epoch 395/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1079 - acc: 0.9678\n",
      "Epoch 396/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1130 - acc: 0.9634\n",
      "Epoch 397/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1155 - acc: 0.9610\n",
      "Epoch 398/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1125 - acc: 0.9632\n",
      "Epoch 399/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1206 - acc: 0.9606\n",
      "Epoch 400/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.0996 - acc: 0.9714\n",
      "Epoch 401/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1034 - acc: 0.9671\n",
      "Epoch 402/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1118 - acc: 0.9617\n",
      "Epoch 403/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1024 - acc: 0.9666\n",
      "Epoch 404/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1130 - acc: 0.9645\n",
      "Epoch 405/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1115 - acc: 0.9646\n",
      "Epoch 406/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1040 - acc: 0.9660\n",
      "Epoch 407/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1131 - acc: 0.9597\n",
      "Epoch 408/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1008 - acc: 0.9660\n",
      "Epoch 409/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1033 - acc: 0.9694\n",
      "Epoch 410/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1120 - acc: 0.9625\n",
      "Epoch 411/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1017 - acc: 0.9681\n",
      "Epoch 412/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1088 - acc: 0.9618\n",
      "Epoch 413/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1147 - acc: 0.9635\n",
      "Epoch 414/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1060 - acc: 0.9656\n",
      "Epoch 415/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1110 - acc: 0.9635\n",
      "Epoch 416/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1098 - acc: 0.9650\n",
      "Epoch 417/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1012 - acc: 0.9670\n",
      "Epoch 418/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1122 - acc: 0.9649\n",
      "Epoch 419/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1070 - acc: 0.9636\n",
      "Epoch 420/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1138 - acc: 0.9622\n",
      "Epoch 421/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1073 - acc: 0.9670\n",
      "Epoch 422/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1031 - acc: 0.9642\n",
      "Epoch 423/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1063 - acc: 0.9649\n",
      "Epoch 424/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1039 - acc: 0.9673\n",
      "Epoch 425/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1200 - acc: 0.9615\n",
      "Epoch 426/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1110 - acc: 0.9625\n",
      "Epoch 427/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1123 - acc: 0.9636\n",
      "Epoch 428/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1110 - acc: 0.9615\n",
      "Epoch 429/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1201 - acc: 0.9588\n",
      "Epoch 430/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1145 - acc: 0.9618\n",
      "Epoch 431/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1121 - acc: 0.9613\n",
      "Epoch 432/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1077 - acc: 0.9641\n",
      "Epoch 433/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1052 - acc: 0.9655\n",
      "Epoch 434/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1042 - acc: 0.9684\n",
      "Epoch 435/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1114 - acc: 0.9621\n",
      "Epoch 436/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1061 - acc: 0.9656\n",
      "Epoch 437/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1109 - acc: 0.9639\n",
      "Epoch 438/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1086 - acc: 0.9645\n",
      "Epoch 439/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1080 - acc: 0.9649\n",
      "Epoch 440/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1053 - acc: 0.9622\n",
      "Epoch 441/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1089 - acc: 0.9638\n",
      "Epoch 442/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1077 - acc: 0.9632\n",
      "Epoch 443/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1089 - acc: 0.9639\n",
      "Epoch 444/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1084 - acc: 0.9667\n",
      "Epoch 445/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1130 - acc: 0.9607\n",
      "Epoch 446/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1127 - acc: 0.9634\n",
      "Epoch 447/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1102 - acc: 0.9667\n",
      "Epoch 448/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1038 - acc: 0.9692\n",
      "Epoch 449/450\n",
      "7178/7178 [==============================] - 1s 148us/step - loss: 0.1087 - acc: 0.9634\n",
      "Epoch 450/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1055 - acc: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3220127ac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3600\n",
    "batch_size = 3000\n",
    "model_tmp.fit(x_test, hw3_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp.save_weights(\"weight_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = model_tmp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(predict_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"train_sub.csv\", 'w', newline='') as csvfile:\n",
    "    csvfile.write('id,label\\n')\n",
    "    for i, v in enumerate(prediction):\n",
    "        csvfile.write('%d,%d\\n' %(i, prediction[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
