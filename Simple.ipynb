{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.layers import Activation, BatchNormalization, Add, Reshape, DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 48, 48, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 24, 24, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_9 (Depthwis (None, 24, 24, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 24, 24, 16)        1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 24, 24, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_10 (Depthwi (None, 12, 12, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 12, 12, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 12, 12, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_11 (Depthwi (None, 6, 6, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 6, 6, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 6, 6, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_12 (Depthwi (None, 3, 3, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 3, 3, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 3, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 1, 1, 7)           455       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 1, 1, 7)           0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 51,415\n",
      "Trainable params: 49,591\n",
      "Non-trainable params: 1,824\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def relu6(x):\n",
    "    \"\"\"Relu 6\n",
    "    \"\"\"\n",
    "    return K.relu(x, max_value=6.0)\n",
    "\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    return LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, alpha, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        alpha: Integer, width multiplier.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    # Depth\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "    # Width\n",
    "    cchannel = int(filters * alpha)\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    x = LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "    x = Conv2D(cchannel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    if r:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        alpha: Integer, width multiplier.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, alpha, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, alpha, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def MobileNetv2(input_shape, k, alpha=1.0):\n",
    "    \"\"\"MobileNetv2\n",
    "    This function defines a MobileNetv2 architectures.\n",
    "    # Arguments\n",
    "        input_shape: An integer or tuple/list of 3 integers, shape\n",
    "            of input tensor.\n",
    "        k: Integer, number of classes.\n",
    "        alpha: Integer, width multiplier, better in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4].\n",
    "    # Returns\n",
    "        MobileNetv2 model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    #first_filters = _make_divisible(32 * alpha, 8)\n",
    "    first_filters = 32\n",
    "    x = _conv_block(inputs, first_filters, (3, 3), strides=(2, 2))\n",
    "\n",
    "    x = _inverted_residual_block(x, 16, (3, 3), t=2, alpha=alpha, strides=1, n=1)\n",
    "    x = _inverted_residual_block(x, 32, (3, 3), t=2, alpha=alpha, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=2, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 96, (3, 3), t=3, alpha=alpha, strides=1, n=2)\n",
    "    x = _inverted_residual_block(x, 128, (3, 3), t=2, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 320, (3, 3), t=6, alpha=alpha, strides=1, n=1)\n",
    "\n",
    "    #if alpha > 1.0:\n",
    "        #last_filters = _make_divisible(1280 * alpha, 8)\n",
    "    #else:\n",
    "        #last_filters = 1280\n",
    "    last_filters = 64\n",
    "    x = _conv_block(x, last_filters, (1, 1), strides=(1, 1))\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, last_filters))(x)\n",
    "    x = Dropout(0.3, name='Dropout')(x)\n",
    "    x = Conv2D(k, (1, 1), padding='same')(x)\n",
    "\n",
    "    x = Activation('softmax', name='softmax')(x)\n",
    "    output = Reshape((k,))(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    # plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = MobileNetv2((48, 48, 1), 7, 1.0)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.load(\"img_train.npy\")\n",
    "y = np.load(\"img_label.npy\")\n",
    "\n",
    "x_train = x[0:int(28709*0.8)]/255\n",
    "y_train = y[0:int(28709*0.8)]\n",
    "x_val = x[int(28709*0.8): 28709]/255\n",
    "y_val = y[int(28709*0.8):28709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'Adam',\n",
    "              metrics=['accuracy'])\n",
    "#print(MODEL.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 129s 86ms/step - loss: 1.7331 - acc: 0.2924 - val_loss: 1.6242 - val_acc: 0.3577\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.5652 - acc: 0.3839 - val_loss: 1.5946 - val_acc: 0.3575\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.4674 - acc: 0.4325 - val_loss: 1.4649 - val_acc: 0.4349\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.4122 - acc: 0.4564 - val_loss: 1.4798 - val_acc: 0.4225\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.3791 - acc: 0.4709 - val_loss: 1.3172 - val_acc: 0.4937\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.3521 - acc: 0.4836 - val_loss: 1.4717 - val_acc: 0.4347\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.3340 - acc: 0.4902 - val_loss: 1.2831 - val_acc: 0.5045\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.3163 - acc: 0.4982 - val_loss: 1.4153 - val_acc: 0.4544\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.3048 - acc: 0.5025 - val_loss: 1.3173 - val_acc: 0.4977\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2908 - acc: 0.5072 - val_loss: 1.2572 - val_acc: 0.5263\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2806 - acc: 0.5131 - val_loss: 1.2869 - val_acc: 0.5070\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2713 - acc: 0.5167 - val_loss: 1.2447 - val_acc: 0.5303\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2625 - acc: 0.5201 - val_loss: 1.1940 - val_acc: 0.5442\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.2528 - acc: 0.5247 - val_loss: 1.2432 - val_acc: 0.5341\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2466 - acc: 0.5259 - val_loss: 1.3015 - val_acc: 0.5044\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 125s 84ms/step - loss: 1.2398 - acc: 0.5295 - val_loss: 1.3455 - val_acc: 0.4842\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2336 - acc: 0.5312 - val_loss: 1.2703 - val_acc: 0.5178\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2304 - acc: 0.5337 - val_loss: 1.2174 - val_acc: 0.5380\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 126s 84ms/step - loss: 1.2214 - acc: 0.5358 - val_loss: 1.2128 - val_acc: 0.5413\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 124s 82ms/step - loss: 1.2183 - acc: 0.5376 - val_loss: 1.2937 - val_acc: 0.5167\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.2112 - acc: 0.5391 - val_loss: 1.1916 - val_acc: 0.5486\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.2087 - acc: 0.5410 - val_loss: 1.1884 - val_acc: 0.5463\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.2046 - acc: 0.5419 - val_loss: 1.2286 - val_acc: 0.5303\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.1986 - acc: 0.5455 - val_loss: 1.1555 - val_acc: 0.5677\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.1948 - acc: 0.5475 - val_loss: 1.2943 - val_acc: 0.5171\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    "    )\n",
    "\n",
    "datagen.fit(x_train)\n",
    "train_generator = datagen.flow(x_train,y_train,batch_size = 200)\n",
    "#learning_rate_function = ReduceLROnPlateau(monitor='val_acc',patience=2,epsilon=0.00001,verbose=1,factor=0.2)\n",
    "train_history2 = model.fit_generator(train_generator, steps_per_epoch=1500,epochs=25,verbose=1,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 48, 48, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 16)        528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 12, 12, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 12, 12, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 12, 12, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_7 (Depthwis (None, 6, 6, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 64)          2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_8 (Depthwis (None, 3, 3, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 3, 3, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 1, 1, 7)           455       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 1, 1, 7)           0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 31,015\n",
      "Trainable params: 29,767\n",
      "Non-trainable params: 1,248\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_no = MobileNetv2((48, 48, 1), 7, 1.0)\n",
    "print(model_no.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22967 samples, validate on 5742 samples\n",
      "Epoch 1/100\n",
      "22967/22967 [==============================] - 18s 774us/step - loss: 1.9753 - acc: 0.1806 - val_loss: 1.8669 - val_acc: 0.2461\n",
      "Epoch 2/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.8165 - acc: 0.2635 - val_loss: 1.7671 - val_acc: 0.2886\n",
      "Epoch 3/100\n",
      "22967/22967 [==============================] - 7s 318us/step - loss: 1.7452 - acc: 0.3000 - val_loss: 1.7489 - val_acc: 0.3042\n",
      "Epoch 4/100\n",
      "22967/22967 [==============================] - 7s 316us/step - loss: 1.6954 - acc: 0.3232 - val_loss: 1.7535 - val_acc: 0.3116\n",
      "Epoch 5/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.6489 - acc: 0.3472 - val_loss: 1.7090 - val_acc: 0.3276\n",
      "Epoch 6/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.6115 - acc: 0.3620 - val_loss: 1.6926 - val_acc: 0.3400\n",
      "Epoch 7/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.5731 - acc: 0.3853 - val_loss: 1.8008 - val_acc: 0.3318\n",
      "Epoch 8/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.5460 - acc: 0.3948 - val_loss: 1.7703 - val_acc: 0.3466\n",
      "Epoch 9/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.5164 - acc: 0.4089 - val_loss: 1.8007 - val_acc: 0.3413\n",
      "Epoch 10/100\n",
      "22967/22967 [==============================] - 7s 324us/step - loss: 1.4891 - acc: 0.4184 - val_loss: 1.6960 - val_acc: 0.3624\n",
      "Epoch 11/100\n",
      "22967/22967 [==============================] - 7s 324us/step - loss: 1.4594 - acc: 0.4366 - val_loss: 1.7714 - val_acc: 0.3488\n",
      "Epoch 12/100\n",
      "22967/22967 [==============================] - 7s 323us/step - loss: 1.4411 - acc: 0.4448 - val_loss: 1.7884 - val_acc: 0.3511\n",
      "Epoch 13/100\n",
      "10000/22967 [============>.................] - ETA: 3s - loss: 1.4239 - acc: 0.4514"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-42a6ae06bb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         validation_data=(x_val,y_val))\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#callbacks=[learning_rate_function])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 2000\n",
    "model_no.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'Adam',\n",
    "              metrics=['accuracy'])\n",
    "model_no.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "        validation_data=(x_val,y_val))\n",
    "        #callbacks=[learning_rate_function])\n",
    "#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#model_no.save('MobileNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(\"MobileNetv2.h5\")\n",
    "model.save_weights(\"Weightv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "n = 7178\n",
    "test_x = []\n",
    "for i in range(n):\n",
    "    test_x.append([])\n",
    "\n",
    "count = 0\n",
    "with open('test.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        if(row[0] != \"id\"):\n",
    "            test_x[count].append(row[1])\n",
    "            count += 1\n",
    "x_test = []\n",
    "for i in range(len(test_x)):\n",
    "    x_test.append(test_x[i][0].split(' '))\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(x_test[0])):\n",
    "        x_test[i][j] = float(x_test[i][j])\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape(7178,48,48,1)\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "hw3_model = load_model(\"hw3_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_predict = hw3_model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_y = np_utils.to_categorical(hw3_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7178/7178 [==============================] - 2s 268us/step - loss: 0.5212 - acc: 0.8154\n",
      "Epoch 2/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.5176 - acc: 0.8136\n",
      "Epoch 3/100\n",
      "7178/7178 [==============================] - 2s 263us/step - loss: 0.5160 - acc: 0.8140\n",
      "Epoch 4/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.5168 - acc: 0.8139\n",
      "Epoch 5/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.5145 - acc: 0.8213\n",
      "Epoch 6/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.5042 - acc: 0.8256\n",
      "Epoch 7/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.5038 - acc: 0.8243\n",
      "Epoch 8/100\n",
      "7178/7178 [==============================] - 2s 260us/step - loss: 0.5025 - acc: 0.8235\n",
      "Epoch 9/100\n",
      "7178/7178 [==============================] - 2s 259us/step - loss: 0.4952 - acc: 0.8288\n",
      "Epoch 10/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.4992 - acc: 0.8218\n",
      "Epoch 11/100\n",
      "7178/7178 [==============================] - 2s 261us/step - loss: 0.4911 - acc: 0.8282\n",
      "Epoch 12/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.4849 - acc: 0.8288\n",
      "Epoch 13/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.4870 - acc: 0.8288\n",
      "Epoch 14/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.4863 - acc: 0.8299\n",
      "Epoch 15/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.4815 - acc: 0.8339\n",
      "Epoch 16/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.4776 - acc: 0.8367\n",
      "Epoch 17/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.4770 - acc: 0.8327\n",
      "Epoch 18/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.4720 - acc: 0.8357\n",
      "Epoch 19/100\n",
      "7178/7178 [==============================] - 2s 260us/step - loss: 0.4662 - acc: 0.8383\n",
      "Epoch 20/100\n",
      "7178/7178 [==============================] - 2s 259us/step - loss: 0.4662 - acc: 0.8348\n",
      "Epoch 21/100\n",
      "7178/7178 [==============================] - 2s 260us/step - loss: 0.4623 - acc: 0.8445\n",
      "Epoch 22/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.4587 - acc: 0.8422\n",
      "Epoch 23/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4553 - acc: 0.8422\n",
      "Epoch 24/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.4508 - acc: 0.8426\n",
      "Epoch 25/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.4470 - acc: 0.8466\n",
      "Epoch 26/100\n",
      "7178/7178 [==============================] - 2s 250us/step - loss: 0.4468 - acc: 0.8484\n",
      "Epoch 27/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.4437 - acc: 0.8476\n",
      "Epoch 28/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.4426 - acc: 0.8488\n",
      "Epoch 29/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.4380 - acc: 0.8505\n",
      "Epoch 30/100\n",
      "7178/7178 [==============================] - 2s 259us/step - loss: 0.4350 - acc: 0.8533\n",
      "Epoch 31/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4319 - acc: 0.8561\n",
      "Epoch 32/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4287 - acc: 0.8568\n",
      "Epoch 33/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4290 - acc: 0.8516\n",
      "Epoch 34/100\n",
      "7178/7178 [==============================] - 2s 260us/step - loss: 0.4221 - acc: 0.8580\n",
      "Epoch 35/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4273 - acc: 0.8537\n",
      "Epoch 36/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.4227 - acc: 0.8579\n",
      "Epoch 37/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.4193 - acc: 0.8614\n",
      "Epoch 38/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.4124 - acc: 0.8663\n",
      "Epoch 39/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.4063 - acc: 0.8628\n",
      "Epoch 40/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.4106 - acc: 0.8619\n",
      "Epoch 41/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.4056 - acc: 0.8665\n",
      "Epoch 42/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.4000 - acc: 0.8699\n",
      "Epoch 43/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3979 - acc: 0.8681\n",
      "Epoch 44/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3993 - acc: 0.8656\n",
      "Epoch 45/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3985 - acc: 0.8686\n",
      "Epoch 46/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.3954 - acc: 0.8717\n",
      "Epoch 47/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.3926 - acc: 0.8735\n",
      "Epoch 48/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.3905 - acc: 0.8718\n",
      "Epoch 49/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.3854 - acc: 0.8742\n",
      "Epoch 50/100\n",
      "7178/7178 [==============================] - 2s 260us/step - loss: 0.3811 - acc: 0.8780\n",
      "Epoch 51/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3799 - acc: 0.8756\n",
      "Epoch 52/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3765 - acc: 0.8748\n",
      "Epoch 53/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.3750 - acc: 0.8756\n",
      "Epoch 54/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3679 - acc: 0.8851\n",
      "Epoch 55/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3693 - acc: 0.8775\n",
      "Epoch 56/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3698 - acc: 0.8782\n",
      "Epoch 57/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3625 - acc: 0.8842\n",
      "Epoch 58/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.3639 - acc: 0.8835\n",
      "Epoch 59/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3545 - acc: 0.8866\n",
      "Epoch 60/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.3558 - acc: 0.8828\n",
      "Epoch 61/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.3542 - acc: 0.8863\n",
      "Epoch 62/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3501 - acc: 0.8873\n",
      "Epoch 63/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.3498 - acc: 0.8877\n",
      "Epoch 64/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.3467 - acc: 0.8885\n",
      "Epoch 65/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.3403 - acc: 0.8905\n",
      "Epoch 66/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.3426 - acc: 0.8919\n",
      "Epoch 67/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.3363 - acc: 0.8955\n",
      "Epoch 68/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.3379 - acc: 0.8941\n",
      "Epoch 69/100\n",
      "7178/7178 [==============================] - 2s 251us/step - loss: 0.3283 - acc: 0.8994\n",
      "Epoch 70/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.3321 - acc: 0.8983\n",
      "Epoch 71/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3285 - acc: 0.8972\n",
      "Epoch 72/100\n",
      "7178/7178 [==============================] - 2s 259us/step - loss: 0.3276 - acc: 0.8984\n",
      "Epoch 73/100\n",
      "7178/7178 [==============================] - 2s 261us/step - loss: 0.3227 - acc: 0.8986\n",
      "Epoch 74/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.3222 - acc: 0.9016\n",
      "Epoch 75/100\n",
      "7178/7178 [==============================] - 2s 261us/step - loss: 0.3188 - acc: 0.9014\n",
      "Epoch 76/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3186 - acc: 0.9011\n",
      "Epoch 77/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.3129 - acc: 0.9047\n",
      "Epoch 78/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.3117 - acc: 0.9029\n",
      "Epoch 79/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.3118 - acc: 0.9042\n",
      "Epoch 80/100\n",
      "7178/7178 [==============================] - 2s 249us/step - loss: 0.3082 - acc: 0.9047\n",
      "Epoch 81/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.3072 - acc: 0.9065\n",
      "Epoch 82/100\n",
      "7178/7178 [==============================] - 2s 262us/step - loss: 0.3013 - acc: 0.9069\n",
      "Epoch 83/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.2989 - acc: 0.9094\n",
      "Epoch 84/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.2946 - acc: 0.9133\n",
      "Epoch 85/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.2980 - acc: 0.9071\n",
      "Epoch 86/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.2945 - acc: 0.9078\n",
      "Epoch 87/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.2871 - acc: 0.9182\n",
      "Epoch 88/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.2871 - acc: 0.9136\n",
      "Epoch 89/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.2850 - acc: 0.9147\n",
      "Epoch 90/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.2876 - acc: 0.9107\n",
      "Epoch 91/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.2797 - acc: 0.9198\n",
      "Epoch 92/100\n",
      "7178/7178 [==============================] - 2s 255us/step - loss: 0.2764 - acc: 0.9211\n",
      "Epoch 93/100\n",
      "7178/7178 [==============================] - 2s 254us/step - loss: 0.2738 - acc: 0.9232\n",
      "Epoch 94/100\n",
      "7178/7178 [==============================] - 2s 252us/step - loss: 0.2787 - acc: 0.9184\n",
      "Epoch 95/100\n",
      "7178/7178 [==============================] - 2s 253us/step - loss: 0.2707 - acc: 0.9211\n",
      "Epoch 96/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.2721 - acc: 0.9195\n",
      "Epoch 97/100\n",
      "7178/7178 [==============================] - 2s 257us/step - loss: 0.2708 - acc: 0.9206\n",
      "Epoch 98/100\n",
      "7178/7178 [==============================] - 2s 258us/step - loss: 0.2672 - acc: 0.9218\n",
      "Epoch 99/100\n",
      "7178/7178 [==============================] - 2s 256us/step - loss: 0.2697 - acc: 0.9209\n",
      "Epoch 100/100\n",
      "7178/7178 [==============================] - 2s 259us/step - loss: 0.2601 - acc: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32a6bfef60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 3000\n",
    "#model_no.compile(loss='categorical_crossentropy',\n",
    "#              optimizer= 'Adam',\n",
    "#              metrics=['accuracy'])\n",
    "model_tmp.fit(x_test, hw3_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)\n",
    "        #callbacks=[learning_rate_function])\n",
    "#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#model_no.save('MobileNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model_tmp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction2 = np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, ..., 3, 0, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp.save(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", 'w', newline='') as csvfile:\n",
    "    csvfile.write('id,label\\n')\n",
    "    for i, v in enumerate(prediction2):\n",
    "        csvfile.write('%d,%d\\n' %(i, prediction2[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
