{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.layers import Activation, BatchNormalization, Add, Reshape, DepthwiseConv2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 48, 48, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_17 (Depthwi (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_18 (Depthwi (None, 12, 12, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 12, 12, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 12, 12, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 12, 12, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_19 (Depthwi (None, 6, 6, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 6, 6, 64)          2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 6, 6, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_20 (Depthwi (None, 3, 3, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 3, 3, 96)          6240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 3, 3, 96)          384       \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 3, 3, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 1, 1, 7)           231       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 1, 1, 7)           0         \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 21,751\n",
      "Trainable params: 20,727\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def relu6(x):\n",
    "    \"\"\"Relu 6\n",
    "    \"\"\"\n",
    "    return K.relu(x, max_value=6.0)\n",
    "\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    return LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, alpha, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        alpha: Integer, width multiplier.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    # Depth\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "    # Width\n",
    "    cchannel = int(filters * alpha)\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    x = LeakyReLU(alpha=0.03)(x)\n",
    "\n",
    "    x = Conv2D(cchannel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    #x = BatchNormalizationF16(axis=channel_axis)(x)\n",
    "    \n",
    "    if r:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        alpha: Integer, width multiplier.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    x = _bottleneck(inputs, filters, kernel, t, alpha, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, alpha, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def MobileNetv2(input_shape, k, alpha=1.0):\n",
    "    \"\"\"MobileNetv2\n",
    "    This function defines a MobileNetv2 architectures.\n",
    "    # Arguments\n",
    "        input_shape: An integer or tuple/list of 3 integers, shape\n",
    "            of input tensor.\n",
    "        k: Integer, number of classes.\n",
    "        alpha: Integer, width multiplier, better in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4].\n",
    "    # Returns\n",
    "        MobileNetv2 model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    #first_filters = _make_divisible(32 * alpha, 8)\n",
    "    first_filters = 16\n",
    "    x = _conv_block(inputs, first_filters, (3, 3), strides=(2, 2))\n",
    "\n",
    "    x = _inverted_residual_block(x, 16, (3, 3), t=1, alpha=alpha, strides=1, n=1)\n",
    "    x = _inverted_residual_block(x, 32, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 96, (3, 3), t=3, alpha=alpha, strides=1, n=2)\n",
    "    x = _inverted_residual_block(x, 96, (3, 3), t=1, alpha=alpha, strides=2, n=1)\n",
    "    #x = _inverted_residual_block(x, 320, (3, 3), t=6, alpha=alpha, strides=1, n=1)\n",
    "\n",
    "    #if alpha > 1.0:\n",
    "        #last_filters = _make_divisible(1280 * alpha, 8)\n",
    "    #else:\n",
    "        #last_filters = 1280\n",
    "    last_filters = 32\n",
    "    x = _conv_block(x, last_filters, (1, 1), strides=(1, 1))\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, last_filters))(x)\n",
    "    x = Dropout(0.3, name='Dropout')(x)\n",
    "    x = Conv2D(k, (1, 1), padding='same')(x)\n",
    "\n",
    "    x = Activation('softmax', name='softmax')(x)\n",
    "    output = Reshape((k,))(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    # plot_model(model, to_file='images/MobileNetv2.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = MobileNetv2((48, 48, 1), 7, 1.0)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.load(\"img_train.npy\")\n",
    "y = np.load(\"img_label.npy\")\n",
    "\n",
    "x_train = x[0:int(28709*0.8)]/255\n",
    "y_train = y[0:int(28709*0.8)]\n",
    "x_val = x[int(28709*0.8): 28709]/255\n",
    "y_val = y[int(28709*0.8):28709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'Adam',\n",
    "              metrics=['accuracy'])\n",
    "#print(MODEL.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 129s 86ms/step - loss: 1.7760 - acc: 0.2645 - val_loss: 1.7223 - val_acc: 0.3013\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.6912 - acc: 0.3171 - val_loss: 1.6134 - val_acc: 0.3631\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 121s 80ms/step - loss: 1.6261 - acc: 0.3566 - val_loss: 1.5877 - val_acc: 0.3743\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.5832 - acc: 0.3797 - val_loss: 1.5873 - val_acc: 0.3831\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.5541 - acc: 0.3942 - val_loss: 1.4814 - val_acc: 0.4260\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 1.5339 - acc: 0.4032 - val_loss: 1.4757 - val_acc: 0.4314\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.5159 - acc: 0.4119 - val_loss: 1.5157 - val_acc: 0.4270\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4992 - acc: 0.4198 - val_loss: 1.5153 - val_acc: 0.4181\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 1.4860 - acc: 0.4257 - val_loss: 1.4410 - val_acc: 0.4516\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4757 - acc: 0.4305 - val_loss: 1.5552 - val_acc: 0.4185\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4646 - acc: 0.4358 - val_loss: 1.4016 - val_acc: 0.4608\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 1.4575 - acc: 0.4379 - val_loss: 1.7544 - val_acc: 0.3579\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4485 - acc: 0.4426 - val_loss: 1.4905 - val_acc: 0.4446\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4438 - acc: 0.4450 - val_loss: 1.3743 - val_acc: 0.4817\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4377 - acc: 0.4468 - val_loss: 1.3960 - val_acc: 0.4552\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4341 - acc: 0.4486 - val_loss: 1.3792 - val_acc: 0.4810\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 122s 82ms/step - loss: 1.4316 - acc: 0.4493 - val_loss: 1.3854 - val_acc: 0.4577\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.4285 - acc: 0.4513 - val_loss: 1.3689 - val_acc: 0.4859\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.4261 - acc: 0.4519 - val_loss: 1.3663 - val_acc: 0.4768\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4201 - acc: 0.4541 - val_loss: 1.3726 - val_acc: 0.4735\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 1.4179 - acc: 0.4552 - val_loss: 1.3586 - val_acc: 0.4767\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 121s 81ms/step - loss: 1.4176 - acc: 0.4549 - val_loss: 1.3752 - val_acc: 0.4751\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 121s 80ms/step - loss: 1.4132 - acc: 0.4584 - val_loss: 1.4434 - val_acc: 0.4411\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 120s 80ms/step - loss: 1.4117 - acc: 0.4583 - val_loss: 1.3179 - val_acc: 0.4963\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 120s 80ms/step - loss: 1.4111 - acc: 0.4599 - val_loss: 1.3695 - val_acc: 0.4735\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    "    )\n",
    "\n",
    "datagen.fit(x_train)\n",
    "train_generator = datagen.flow(x_train,y_train,batch_size = 200)\n",
    "#learning_rate_function = ReduceLROnPlateau(monitor='val_acc',patience=2,epsilon=0.00001,verbose=1,factor=0.2)\n",
    "train_history2 = model.fit_generator(train_generator, steps_per_epoch=1500,epochs=25,verbose=1,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 48, 48, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 16)        528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 12, 12, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 12, 12, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 12, 12, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_7 (Depthwis (None, 6, 6, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 64)          2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_8 (Depthwis (None, 3, 3, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 3, 3, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 1, 1, 7)           455       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 1, 1, 7)           0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 31,015\n",
      "Trainable params: 29,767\n",
      "Non-trainable params: 1,248\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_no = MobileNetv2((48, 48, 1), 7, 1.0)\n",
    "print(model_no.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22967 samples, validate on 5742 samples\n",
      "Epoch 1/100\n",
      "22967/22967 [==============================] - 18s 774us/step - loss: 1.9753 - acc: 0.1806 - val_loss: 1.8669 - val_acc: 0.2461\n",
      "Epoch 2/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.8165 - acc: 0.2635 - val_loss: 1.7671 - val_acc: 0.2886\n",
      "Epoch 3/100\n",
      "22967/22967 [==============================] - 7s 318us/step - loss: 1.7452 - acc: 0.3000 - val_loss: 1.7489 - val_acc: 0.3042\n",
      "Epoch 4/100\n",
      "22967/22967 [==============================] - 7s 316us/step - loss: 1.6954 - acc: 0.3232 - val_loss: 1.7535 - val_acc: 0.3116\n",
      "Epoch 5/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.6489 - acc: 0.3472 - val_loss: 1.7090 - val_acc: 0.3276\n",
      "Epoch 6/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.6115 - acc: 0.3620 - val_loss: 1.6926 - val_acc: 0.3400\n",
      "Epoch 7/100\n",
      "22967/22967 [==============================] - 7s 320us/step - loss: 1.5731 - acc: 0.3853 - val_loss: 1.8008 - val_acc: 0.3318\n",
      "Epoch 8/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.5460 - acc: 0.3948 - val_loss: 1.7703 - val_acc: 0.3466\n",
      "Epoch 9/100\n",
      "22967/22967 [==============================] - 7s 322us/step - loss: 1.5164 - acc: 0.4089 - val_loss: 1.8007 - val_acc: 0.3413\n",
      "Epoch 10/100\n",
      "22967/22967 [==============================] - 7s 324us/step - loss: 1.4891 - acc: 0.4184 - val_loss: 1.6960 - val_acc: 0.3624\n",
      "Epoch 11/100\n",
      "22967/22967 [==============================] - 7s 324us/step - loss: 1.4594 - acc: 0.4366 - val_loss: 1.7714 - val_acc: 0.3488\n",
      "Epoch 12/100\n",
      "22967/22967 [==============================] - 7s 323us/step - loss: 1.4411 - acc: 0.4448 - val_loss: 1.7884 - val_acc: 0.3511\n",
      "Epoch 13/100\n",
      "10000/22967 [============>.................] - ETA: 3s - loss: 1.4239 - acc: 0.4514"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-42a6ae06bb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         validation_data=(x_val,y_val))\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#callbacks=[learning_rate_function])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 2000\n",
    "model_no.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'Adam',\n",
    "              metrics=['accuracy'])\n",
    "model_no.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "        validation_data=(x_val,y_val))\n",
    "        #callbacks=[learning_rate_function])\n",
    "#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#model_no.save('MobileNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "n = 7178\n",
    "test_x = []\n",
    "for i in range(n):\n",
    "    test_x.append([])\n",
    "\n",
    "count = 0\n",
    "with open('test.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        if(row[0] != \"id\"):\n",
    "            test_x[count].append(row[1])\n",
    "            count += 1\n",
    "x_test = []\n",
    "for i in range(len(test_x)):\n",
    "    x_test.append(test_x[i][0].split(' '))\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(x_test[0])):\n",
    "        x_test[i][j] = float(x_test[i][j])\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape(7178,48,48,1)\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/g4gpeanut/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "hw3_model = load_model(\"hw3_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_predict = hw3_model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hw3_y = np_utils.to_categorical(hw3_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "7178/7178 [==============================] - 1s 175us/step - loss: 0.1389 - acc: 0.9550\n",
      "Epoch 2/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1414 - acc: 0.9558\n",
      "Epoch 3/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1353 - acc: 0.9576\n",
      "Epoch 4/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1390 - acc: 0.9560\n",
      "Epoch 5/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1380 - acc: 0.9561\n",
      "Epoch 6/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1405 - acc: 0.9557\n",
      "Epoch 7/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1337 - acc: 0.9568\n",
      "Epoch 8/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1362 - acc: 0.9550\n",
      "Epoch 9/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1340 - acc: 0.9578\n",
      "Epoch 10/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1398 - acc: 0.9565\n",
      "Epoch 11/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1408 - acc: 0.9532\n",
      "Epoch 12/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1342 - acc: 0.9585\n",
      "Epoch 13/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1330 - acc: 0.9567\n",
      "Epoch 14/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1394 - acc: 0.9579\n",
      "Epoch 15/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1297 - acc: 0.9600\n",
      "Epoch 16/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1449 - acc: 0.9517\n",
      "Epoch 17/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1326 - acc: 0.9599\n",
      "Epoch 18/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1343 - acc: 0.9553\n",
      "Epoch 19/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1378 - acc: 0.9563\n",
      "Epoch 20/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1365 - acc: 0.9565\n",
      "Epoch 21/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1283 - acc: 0.9603\n",
      "Epoch 22/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1412 - acc: 0.9551\n",
      "Epoch 23/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1349 - acc: 0.9546\n",
      "Epoch 24/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1315 - acc: 0.9603\n",
      "Epoch 25/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1437 - acc: 0.9537\n",
      "Epoch 26/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1328 - acc: 0.9581\n",
      "Epoch 27/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1365 - acc: 0.9554\n",
      "Epoch 28/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1361 - acc: 0.9556\n",
      "Epoch 29/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1316 - acc: 0.9585\n",
      "Epoch 30/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1375 - acc: 0.9554\n",
      "Epoch 31/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1333 - acc: 0.9602\n",
      "Epoch 32/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1419 - acc: 0.9560\n",
      "Epoch 33/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1400 - acc: 0.9551\n",
      "Epoch 34/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1290 - acc: 0.9613\n",
      "Epoch 35/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1328 - acc: 0.9582\n",
      "Epoch 36/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1422 - acc: 0.9539\n",
      "Epoch 37/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1389 - acc: 0.9554\n",
      "Epoch 38/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1348 - acc: 0.9578\n",
      "Epoch 39/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1407 - acc: 0.9539\n",
      "Epoch 40/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1348 - acc: 0.9568\n",
      "Epoch 41/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1341 - acc: 0.9565\n",
      "Epoch 42/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1294 - acc: 0.9625\n",
      "Epoch 43/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1347 - acc: 0.9572\n",
      "Epoch 44/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1342 - acc: 0.9558\n",
      "Epoch 45/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1277 - acc: 0.9600\n",
      "Epoch 46/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1329 - acc: 0.9572\n",
      "Epoch 47/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1420 - acc: 0.9526\n",
      "Epoch 48/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1450 - acc: 0.9522\n",
      "Epoch 49/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1346 - acc: 0.9606\n",
      "Epoch 50/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1428 - acc: 0.9528\n",
      "Epoch 51/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1370 - acc: 0.9546\n",
      "Epoch 52/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1401 - acc: 0.9570\n",
      "Epoch 53/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1359 - acc: 0.9546\n",
      "Epoch 54/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1367 - acc: 0.9539\n",
      "Epoch 55/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1385 - acc: 0.9572\n",
      "Epoch 56/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1417 - acc: 0.9549\n",
      "Epoch 57/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1322 - acc: 0.9568\n",
      "Epoch 58/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1379 - acc: 0.9561\n",
      "Epoch 59/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1428 - acc: 0.9536\n",
      "Epoch 60/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1395 - acc: 0.9561\n",
      "Epoch 61/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1351 - acc: 0.9544\n",
      "Epoch 62/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1343 - acc: 0.9572\n",
      "Epoch 63/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1406 - acc: 0.9550\n",
      "Epoch 64/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1353 - acc: 0.9564\n",
      "Epoch 65/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1378 - acc: 0.9570\n",
      "Epoch 66/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1357 - acc: 0.9570\n",
      "Epoch 67/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1298 - acc: 0.9585\n",
      "Epoch 68/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1327 - acc: 0.9574\n",
      "Epoch 69/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1376 - acc: 0.9572\n",
      "Epoch 70/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1316 - acc: 0.9576\n",
      "Epoch 71/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1344 - acc: 0.9602\n",
      "Epoch 72/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1350 - acc: 0.9590\n",
      "Epoch 73/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1336 - acc: 0.9563\n",
      "Epoch 74/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1361 - acc: 0.9543\n",
      "Epoch 75/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1332 - acc: 0.9595\n",
      "Epoch 76/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1319 - acc: 0.9576\n",
      "Epoch 77/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1372 - acc: 0.9557\n",
      "Epoch 78/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1359 - acc: 0.9588\n",
      "Epoch 79/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1305 - acc: 0.9602\n",
      "Epoch 80/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1357 - acc: 0.9575\n",
      "Epoch 81/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1431 - acc: 0.9529\n",
      "Epoch 82/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1379 - acc: 0.9578\n",
      "Epoch 83/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1368 - acc: 0.9561\n",
      "Epoch 84/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1376 - acc: 0.9563\n",
      "Epoch 85/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1392 - acc: 0.9544\n",
      "Epoch 86/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1455 - acc: 0.9501\n",
      "Epoch 87/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1370 - acc: 0.9540\n",
      "Epoch 88/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1419 - acc: 0.9535\n",
      "Epoch 89/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1376 - acc: 0.9547\n",
      "Epoch 90/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1382 - acc: 0.9570\n",
      "Epoch 91/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1485 - acc: 0.9490\n",
      "Epoch 92/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1372 - acc: 0.9535\n",
      "Epoch 93/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1392 - acc: 0.9542\n",
      "Epoch 94/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1303 - acc: 0.9570\n",
      "Epoch 95/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1440 - acc: 0.9554\n",
      "Epoch 96/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1361 - acc: 0.9550\n",
      "Epoch 97/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1406 - acc: 0.9544\n",
      "Epoch 98/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1364 - acc: 0.9553\n",
      "Epoch 99/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1357 - acc: 0.9553\n",
      "Epoch 100/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1315 - acc: 0.9589\n",
      "Epoch 101/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1290 - acc: 0.9581\n",
      "Epoch 102/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1371 - acc: 0.9528\n",
      "Epoch 103/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1450 - acc: 0.9532\n",
      "Epoch 104/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1360 - acc: 0.9564\n",
      "Epoch 105/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1398 - acc: 0.9556\n",
      "Epoch 106/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1406 - acc: 0.9532\n",
      "Epoch 107/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1382 - acc: 0.9547\n",
      "Epoch 108/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1416 - acc: 0.9531\n",
      "Epoch 109/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1404 - acc: 0.9557\n",
      "Epoch 110/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1350 - acc: 0.9546\n",
      "Epoch 111/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1336 - acc: 0.9561\n",
      "Epoch 112/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1408 - acc: 0.9515\n",
      "Epoch 113/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1309 - acc: 0.9590\n",
      "Epoch 114/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1392 - acc: 0.9543\n",
      "Epoch 115/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1403 - acc: 0.9537\n",
      "Epoch 116/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1411 - acc: 0.9537\n",
      "Epoch 117/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1383 - acc: 0.9547\n",
      "Epoch 118/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1356 - acc: 0.9553\n",
      "Epoch 119/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1406 - acc: 0.9525\n",
      "Epoch 120/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1366 - acc: 0.9561\n",
      "Epoch 121/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1418 - acc: 0.9501\n",
      "Epoch 122/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1330 - acc: 0.9567\n",
      "Epoch 123/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1415 - acc: 0.9522\n",
      "Epoch 124/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1348 - acc: 0.9570\n",
      "Epoch 125/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1378 - acc: 0.9549\n",
      "Epoch 126/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1333 - acc: 0.9572\n",
      "Epoch 127/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1315 - acc: 0.9579\n",
      "Epoch 128/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1284 - acc: 0.9575\n",
      "Epoch 129/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1342 - acc: 0.9575\n",
      "Epoch 130/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1453 - acc: 0.9500\n",
      "Epoch 131/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1343 - acc: 0.9586\n",
      "Epoch 132/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1367 - acc: 0.9554\n",
      "Epoch 133/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1384 - acc: 0.9549\n",
      "Epoch 134/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1397 - acc: 0.9568\n",
      "Epoch 135/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1238 - acc: 0.9597\n",
      "Epoch 136/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1357 - acc: 0.9540\n",
      "Epoch 137/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1370 - acc: 0.9572\n",
      "Epoch 138/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1320 - acc: 0.9567\n",
      "Epoch 139/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1418 - acc: 0.9547\n",
      "Epoch 140/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1299 - acc: 0.9617\n",
      "Epoch 141/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1283 - acc: 0.9578\n",
      "Epoch 142/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1358 - acc: 0.9553\n",
      "Epoch 143/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1339 - acc: 0.9575\n",
      "Epoch 144/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1413 - acc: 0.9539\n",
      "Epoch 145/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1352 - acc: 0.9556\n",
      "Epoch 146/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1436 - acc: 0.9561\n",
      "Epoch 147/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1412 - acc: 0.9549\n",
      "Epoch 148/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1419 - acc: 0.9514\n",
      "Epoch 149/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1426 - acc: 0.9524\n",
      "Epoch 150/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1388 - acc: 0.9535\n",
      "Epoch 151/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1452 - acc: 0.9514\n",
      "Epoch 152/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1331 - acc: 0.9558\n",
      "Epoch 153/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1387 - acc: 0.9558\n",
      "Epoch 154/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1359 - acc: 0.9553\n",
      "Epoch 155/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1339 - acc: 0.9568\n",
      "Epoch 156/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1327 - acc: 0.9553\n",
      "Epoch 157/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1227 - acc: 0.9617\n",
      "Epoch 158/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1312 - acc: 0.9588\n",
      "Epoch 159/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1305 - acc: 0.9588\n",
      "Epoch 160/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1235 - acc: 0.9602\n",
      "Epoch 161/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1258 - acc: 0.9575\n",
      "Epoch 162/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1379 - acc: 0.9567\n",
      "Epoch 163/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1333 - acc: 0.9592\n",
      "Epoch 164/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1359 - acc: 0.9589\n",
      "Epoch 165/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1268 - acc: 0.9568\n",
      "Epoch 166/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1389 - acc: 0.9539\n",
      "Epoch 167/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1453 - acc: 0.9479\n",
      "Epoch 168/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1297 - acc: 0.9574\n",
      "Epoch 169/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1317 - acc: 0.9597\n",
      "Epoch 170/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1381 - acc: 0.9553\n",
      "Epoch 171/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1356 - acc: 0.9561\n",
      "Epoch 172/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1373 - acc: 0.9546\n",
      "Epoch 173/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1323 - acc: 0.9568\n",
      "Epoch 174/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1356 - acc: 0.9557\n",
      "Epoch 175/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1382 - acc: 0.9574\n",
      "Epoch 176/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1416 - acc: 0.9547\n",
      "Epoch 177/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1306 - acc: 0.9550\n",
      "Epoch 178/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1282 - acc: 0.9581\n",
      "Epoch 179/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1390 - acc: 0.9567\n",
      "Epoch 180/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1306 - acc: 0.9581\n",
      "Epoch 181/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1186 - acc: 0.9641\n",
      "Epoch 182/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1290 - acc: 0.9590\n",
      "Epoch 183/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1422 - acc: 0.9533\n",
      "Epoch 184/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1432 - acc: 0.9533\n",
      "Epoch 185/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1317 - acc: 0.9582\n",
      "Epoch 186/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1361 - acc: 0.9547\n",
      "Epoch 187/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1309 - acc: 0.9582\n",
      "Epoch 188/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1335 - acc: 0.9551\n",
      "Epoch 189/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1278 - acc: 0.9586\n",
      "Epoch 190/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1223 - acc: 0.9599\n",
      "Epoch 191/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1333 - acc: 0.9599\n",
      "Epoch 192/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1311 - acc: 0.9571\n",
      "Epoch 193/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1302 - acc: 0.9592\n",
      "Epoch 194/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1304 - acc: 0.9581\n",
      "Epoch 195/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1325 - acc: 0.9578\n",
      "Epoch 196/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1358 - acc: 0.9567\n",
      "Epoch 197/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1275 - acc: 0.9593\n",
      "Epoch 198/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1374 - acc: 0.9567\n",
      "Epoch 199/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1362 - acc: 0.9539\n",
      "Epoch 200/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1429 - acc: 0.9524\n",
      "Epoch 201/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1362 - acc: 0.9599\n",
      "Epoch 202/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1445 - acc: 0.9532\n",
      "Epoch 203/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1369 - acc: 0.9568\n",
      "Epoch 204/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1432 - acc: 0.9543\n",
      "Epoch 205/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1404 - acc: 0.9531\n",
      "Epoch 206/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1368 - acc: 0.9532\n",
      "Epoch 207/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1330 - acc: 0.9558\n",
      "Epoch 208/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1376 - acc: 0.9549\n",
      "Epoch 209/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1306 - acc: 0.9593\n",
      "Epoch 210/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1392 - acc: 0.9567\n",
      "Epoch 211/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1327 - acc: 0.9585\n",
      "Epoch 212/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1301 - acc: 0.9581\n",
      "Epoch 213/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1426 - acc: 0.9533\n",
      "Epoch 214/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1355 - acc: 0.9564\n",
      "Epoch 215/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1395 - acc: 0.9551\n",
      "Epoch 216/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1433 - acc: 0.9528\n",
      "Epoch 217/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1333 - acc: 0.9553\n",
      "Epoch 218/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1248 - acc: 0.9606\n",
      "Epoch 219/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1331 - acc: 0.9558\n",
      "Epoch 220/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1370 - acc: 0.9564\n",
      "Epoch 221/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1340 - acc: 0.9571\n",
      "Epoch 222/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1373 - acc: 0.9536\n",
      "Epoch 223/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1348 - acc: 0.9564\n",
      "Epoch 224/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1321 - acc: 0.9582\n",
      "Epoch 225/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1328 - acc: 0.9581\n",
      "Epoch 226/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1185 - acc: 0.9627\n",
      "Epoch 227/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1299 - acc: 0.9572\n",
      "Epoch 228/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1278 - acc: 0.9589\n",
      "Epoch 229/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1268 - acc: 0.9589\n",
      "Epoch 230/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1305 - acc: 0.9576\n",
      "Epoch 231/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1294 - acc: 0.9602\n",
      "Epoch 232/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1370 - acc: 0.9550\n",
      "Epoch 233/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1316 - acc: 0.9599\n",
      "Epoch 234/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1300 - acc: 0.9592\n",
      "Epoch 235/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1285 - acc: 0.9583\n",
      "Epoch 236/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1266 - acc: 0.9606\n",
      "Epoch 237/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1303 - acc: 0.9581\n",
      "Epoch 238/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1314 - acc: 0.9575\n",
      "Epoch 239/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1251 - acc: 0.9617\n",
      "Epoch 240/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1203 - acc: 0.9618\n",
      "Epoch 241/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1313 - acc: 0.9561\n",
      "Epoch 242/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1270 - acc: 0.9609\n",
      "Epoch 243/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1324 - acc: 0.9560\n",
      "Epoch 244/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1283 - acc: 0.9610\n",
      "Epoch 245/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1300 - acc: 0.9574\n",
      "Epoch 246/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1274 - acc: 0.9572\n",
      "Epoch 247/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1314 - acc: 0.9600\n",
      "Epoch 248/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1370 - acc: 0.9556\n",
      "Epoch 249/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1345 - acc: 0.9560\n",
      "Epoch 250/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1389 - acc: 0.9561\n",
      "Epoch 251/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1374 - acc: 0.9517\n",
      "Epoch 252/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1420 - acc: 0.9519\n",
      "Epoch 253/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1356 - acc: 0.9553\n",
      "Epoch 254/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1356 - acc: 0.9579\n",
      "Epoch 255/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1328 - acc: 0.9558\n",
      "Epoch 256/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1354 - acc: 0.9582\n",
      "Epoch 257/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1288 - acc: 0.9590\n",
      "Epoch 258/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1252 - acc: 0.9593\n",
      "Epoch 259/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1331 - acc: 0.9558\n",
      "Epoch 260/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1295 - acc: 0.9581\n",
      "Epoch 261/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1365 - acc: 0.9574\n",
      "Epoch 262/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1285 - acc: 0.9583\n",
      "Epoch 263/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1358 - acc: 0.9550\n",
      "Epoch 264/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1293 - acc: 0.9557\n",
      "Epoch 265/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1358 - acc: 0.9551\n",
      "Epoch 266/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1257 - acc: 0.9617\n",
      "Epoch 267/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1331 - acc: 0.9571\n",
      "Epoch 268/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1265 - acc: 0.9592\n",
      "Epoch 269/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1298 - acc: 0.9544\n",
      "Epoch 270/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1304 - acc: 0.9590\n",
      "Epoch 271/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1309 - acc: 0.9589\n",
      "Epoch 272/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1307 - acc: 0.9581\n",
      "Epoch 273/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1236 - acc: 0.9600\n",
      "Epoch 274/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1268 - acc: 0.9585\n",
      "Epoch 275/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1288 - acc: 0.9582\n",
      "Epoch 276/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1298 - acc: 0.9595\n",
      "Epoch 277/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1362 - acc: 0.9542\n",
      "Epoch 278/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1308 - acc: 0.9571\n",
      "Epoch 279/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1328 - acc: 0.9549\n",
      "Epoch 280/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1359 - acc: 0.9560\n",
      "Epoch 281/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1304 - acc: 0.9585\n",
      "Epoch 282/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1240 - acc: 0.9586\n",
      "Epoch 283/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1266 - acc: 0.9629\n",
      "Epoch 284/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1272 - acc: 0.9588\n",
      "Epoch 285/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1291 - acc: 0.9592\n",
      "Epoch 286/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1278 - acc: 0.9585\n",
      "Epoch 287/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1355 - acc: 0.9560\n",
      "Epoch 288/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1323 - acc: 0.9572\n",
      "Epoch 289/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1297 - acc: 0.9593\n",
      "Epoch 290/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1281 - acc: 0.9597\n",
      "Epoch 291/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1222 - acc: 0.9607\n",
      "Epoch 292/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1230 - acc: 0.9600\n",
      "Epoch 293/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1196 - acc: 0.9631\n",
      "Epoch 294/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1232 - acc: 0.9610\n",
      "Epoch 295/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1189 - acc: 0.9624\n",
      "Epoch 296/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1186 - acc: 0.9618\n",
      "Epoch 297/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1215 - acc: 0.9610\n",
      "Epoch 298/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1249 - acc: 0.9595\n",
      "Epoch 299/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1277 - acc: 0.9611\n",
      "Epoch 300/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1307 - acc: 0.9579\n",
      "Epoch 301/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1251 - acc: 0.9586\n",
      "Epoch 302/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1216 - acc: 0.9611\n",
      "Epoch 303/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1280 - acc: 0.9593\n",
      "Epoch 304/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1236 - acc: 0.9604\n",
      "Epoch 305/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1312 - acc: 0.9568\n",
      "Epoch 306/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1319 - acc: 0.9575\n",
      "Epoch 307/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1239 - acc: 0.9622\n",
      "Epoch 308/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1273 - acc: 0.9589\n",
      "Epoch 309/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1315 - acc: 0.9578\n",
      "Epoch 310/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1217 - acc: 0.9639\n",
      "Epoch 311/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1301 - acc: 0.9592\n",
      "Epoch 312/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1270 - acc: 0.9596\n",
      "Epoch 313/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1256 - acc: 0.9589\n",
      "Epoch 314/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1290 - acc: 0.9565\n",
      "Epoch 315/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1258 - acc: 0.9618\n",
      "Epoch 316/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1244 - acc: 0.9595\n",
      "Epoch 317/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1213 - acc: 0.9610\n",
      "Epoch 318/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1282 - acc: 0.9589\n",
      "Epoch 319/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1238 - acc: 0.9595\n",
      "Epoch 320/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1299 - acc: 0.9578\n",
      "Epoch 321/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1235 - acc: 0.9631\n",
      "Epoch 322/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1376 - acc: 0.9539\n",
      "Epoch 323/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1312 - acc: 0.9533\n",
      "Epoch 324/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1228 - acc: 0.9592\n",
      "Epoch 325/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1320 - acc: 0.9590\n",
      "Epoch 326/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1303 - acc: 0.9575\n",
      "Epoch 327/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1291 - acc: 0.9585\n",
      "Epoch 328/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1291 - acc: 0.9554\n",
      "Epoch 329/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1288 - acc: 0.9583\n",
      "Epoch 330/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1274 - acc: 0.9551\n",
      "Epoch 331/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1343 - acc: 0.9576\n",
      "Epoch 332/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1272 - acc: 0.9585\n",
      "Epoch 333/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1236 - acc: 0.9576\n",
      "Epoch 334/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1221 - acc: 0.9641\n",
      "Epoch 335/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1304 - acc: 0.9578\n",
      "Epoch 336/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1410 - acc: 0.9543\n",
      "Epoch 337/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1297 - acc: 0.9572\n",
      "Epoch 338/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1250 - acc: 0.9599\n",
      "Epoch 339/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1306 - acc: 0.9571\n",
      "Epoch 340/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1389 - acc: 0.9551\n",
      "Epoch 341/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1326 - acc: 0.9565\n",
      "Epoch 342/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1234 - acc: 0.9596\n",
      "Epoch 343/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1256 - acc: 0.9592\n",
      "Epoch 344/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1256 - acc: 0.9576\n",
      "Epoch 345/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1383 - acc: 0.9525\n",
      "Epoch 346/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1257 - acc: 0.9604\n",
      "Epoch 347/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1327 - acc: 0.9571\n",
      "Epoch 348/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1243 - acc: 0.9620\n",
      "Epoch 349/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1272 - acc: 0.9572\n",
      "Epoch 350/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1335 - acc: 0.9575\n",
      "Epoch 351/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1225 - acc: 0.9628\n",
      "Epoch 352/450\n",
      "7178/7178 [==============================] - 1s 149us/step - loss: 0.1243 - acc: 0.9622\n",
      "Epoch 353/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1274 - acc: 0.9585\n",
      "Epoch 354/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1178 - acc: 0.9628\n",
      "Epoch 355/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1220 - acc: 0.9615\n",
      "Epoch 356/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1287 - acc: 0.9590\n",
      "Epoch 357/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1279 - acc: 0.9590\n",
      "Epoch 358/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1208 - acc: 0.9595\n",
      "Epoch 359/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1374 - acc: 0.9540\n",
      "Epoch 360/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1338 - acc: 0.9568\n",
      "Epoch 361/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1289 - acc: 0.9585\n",
      "Epoch 362/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1249 - acc: 0.9595\n",
      "Epoch 363/450\n",
      "7178/7178 [==============================] - 1s 159us/step - loss: 0.1230 - acc: 0.9599\n",
      "Epoch 364/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1339 - acc: 0.9572\n",
      "Epoch 365/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1358 - acc: 0.9537\n",
      "Epoch 366/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1333 - acc: 0.9563\n",
      "Epoch 367/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1240 - acc: 0.9600\n",
      "Epoch 368/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1311 - acc: 0.9589\n",
      "Epoch 369/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1288 - acc: 0.9576\n",
      "Epoch 370/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1279 - acc: 0.9588\n",
      "Epoch 371/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1229 - acc: 0.9596\n",
      "Epoch 372/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1305 - acc: 0.9576\n",
      "Epoch 373/450\n",
      "7178/7178 [==============================] - 1s 151us/step - loss: 0.1247 - acc: 0.9625\n",
      "Epoch 374/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1311 - acc: 0.9586\n",
      "Epoch 375/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1291 - acc: 0.9597\n",
      "Epoch 376/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1252 - acc: 0.9592\n",
      "Epoch 377/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1272 - acc: 0.9599\n",
      "Epoch 378/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1273 - acc: 0.9565\n",
      "Epoch 379/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1341 - acc: 0.9571\n",
      "Epoch 380/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1264 - acc: 0.9593\n",
      "Epoch 381/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1282 - acc: 0.9606\n",
      "Epoch 382/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1336 - acc: 0.9567\n",
      "Epoch 383/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1288 - acc: 0.9570\n",
      "Epoch 384/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1264 - acc: 0.9582\n",
      "Epoch 385/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1327 - acc: 0.9549\n",
      "Epoch 386/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1345 - acc: 0.9542\n",
      "Epoch 387/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1196 - acc: 0.9631\n",
      "Epoch 388/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1204 - acc: 0.9631\n",
      "Epoch 389/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1172 - acc: 0.9639\n",
      "Epoch 390/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1316 - acc: 0.9564\n",
      "Epoch 391/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1294 - acc: 0.9590\n",
      "Epoch 392/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1341 - acc: 0.9595\n",
      "Epoch 393/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1333 - acc: 0.9563\n",
      "Epoch 394/450\n",
      "7178/7178 [==============================] - 1s 150us/step - loss: 0.1344 - acc: 0.9556\n",
      "Epoch 395/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1239 - acc: 0.9572\n",
      "Epoch 396/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1281 - acc: 0.9604\n",
      "Epoch 397/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1265 - acc: 0.9595\n",
      "Epoch 398/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1258 - acc: 0.9589\n",
      "Epoch 399/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1332 - acc: 0.9557\n",
      "Epoch 400/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1194 - acc: 0.9641\n",
      "Epoch 401/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1336 - acc: 0.9563\n",
      "Epoch 402/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1248 - acc: 0.9606\n",
      "Epoch 403/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1272 - acc: 0.9583\n",
      "Epoch 404/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1265 - acc: 0.9567\n",
      "Epoch 405/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1273 - acc: 0.9600\n",
      "Epoch 406/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1257 - acc: 0.9617\n",
      "Epoch 407/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1238 - acc: 0.9592\n",
      "Epoch 408/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1263 - acc: 0.9583\n",
      "Epoch 409/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1274 - acc: 0.9581\n",
      "Epoch 410/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1178 - acc: 0.9629\n",
      "Epoch 411/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1302 - acc: 0.9554\n",
      "Epoch 412/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1300 - acc: 0.9595\n",
      "Epoch 413/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1283 - acc: 0.9578\n",
      "Epoch 414/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1282 - acc: 0.9589\n",
      "Epoch 415/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1338 - acc: 0.9563\n",
      "Epoch 416/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1220 - acc: 0.9602\n",
      "Epoch 417/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1159 - acc: 0.9624\n",
      "Epoch 418/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1186 - acc: 0.9617\n",
      "Epoch 419/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1184 - acc: 0.9631\n",
      "Epoch 420/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1276 - acc: 0.9603\n",
      "Epoch 421/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1276 - acc: 0.9578\n",
      "Epoch 422/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1242 - acc: 0.9632\n",
      "Epoch 423/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1167 - acc: 0.9622\n",
      "Epoch 424/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1237 - acc: 0.9602\n",
      "Epoch 425/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1246 - acc: 0.9603\n",
      "Epoch 426/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1289 - acc: 0.9558\n",
      "Epoch 427/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1273 - acc: 0.9592\n",
      "Epoch 428/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1225 - acc: 0.9599\n",
      "Epoch 429/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1205 - acc: 0.9631\n",
      "Epoch 430/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1197 - acc: 0.9599\n",
      "Epoch 431/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1203 - acc: 0.9613\n",
      "Epoch 432/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1227 - acc: 0.9627\n",
      "Epoch 433/450\n",
      "7178/7178 [==============================] - 1s 153us/step - loss: 0.1253 - acc: 0.9611\n",
      "Epoch 434/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1256 - acc: 0.9578\n",
      "Epoch 435/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1192 - acc: 0.9624\n",
      "Epoch 436/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1309 - acc: 0.9578\n",
      "Epoch 437/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1157 - acc: 0.9624\n",
      "Epoch 438/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1308 - acc: 0.9561\n",
      "Epoch 439/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1202 - acc: 0.9638\n",
      "Epoch 440/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1245 - acc: 0.9585\n",
      "Epoch 441/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1207 - acc: 0.9620\n",
      "Epoch 442/450\n",
      "7178/7178 [==============================] - 1s 155us/step - loss: 0.1237 - acc: 0.9611\n",
      "Epoch 443/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1238 - acc: 0.9613\n",
      "Epoch 444/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1174 - acc: 0.9624\n",
      "Epoch 445/450\n",
      "7178/7178 [==============================] - 1s 157us/step - loss: 0.1342 - acc: 0.9549\n",
      "Epoch 446/450\n",
      "7178/7178 [==============================] - 1s 156us/step - loss: 0.1317 - acc: 0.9596\n",
      "Epoch 447/450\n",
      "7178/7178 [==============================] - 1s 158us/step - loss: 0.1346 - acc: 0.9529\n",
      "Epoch 448/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1284 - acc: 0.9588\n",
      "Epoch 449/450\n",
      "7178/7178 [==============================] - 1s 152us/step - loss: 0.1268 - acc: 0.9604\n",
      "Epoch 450/450\n",
      "7178/7178 [==============================] - 1s 154us/step - loss: 0.1276 - acc: 0.9567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30d5c39e10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 450\n",
    "batch_size = 3000\n",
    "#model_no.compile(loss='categorical_crossentropy',\n",
    "#              optimizer= 'Adam',\n",
    "#              metrics=['accuracy'])\n",
    "model_tmp.fit(x_test, hw3_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)\n",
    "        #callbacks=[learning_rate_function])\n",
    "#score = MODEL.model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#model_no.save('MobileNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model_tmp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction2 = np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, ..., 3, 0, 2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tmp.save(\"strong.h5\")\n",
    "model_tmp.save_weights(\"weight_strong.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"submission_strong.csv\", 'w', newline='') as csvfile:\n",
    "    csvfile.write('id,label\\n')\n",
    "    for i, v in enumerate(prediction2):\n",
    "        csvfile.write('%d,%d\\n' %(i, prediction2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
